# postgresql-plotly

sql 학습을 최근에 너무 하지 않은탓에 대부분 잊어버린 것 같아 이번에는 postgresql을 이용해보기로 하였다.
이전에는 oracle이나 pyspark를 사용해서 해보았지만 이번에는 postgresql을 jupyter notebook과 연동하여 사용해보기로 한다. 
시각화는 이전에 사용하였던 plotly를 복습겸 사용하고자 한다.

사실 이전에 사용하였던 pyspark도 복습겸 사용해보고 싶었으나, postgresql과 pyspark연동이 계속 오류때문에 되지 않는다.
사용할 데이터셋은 North-Wind 상거래 데이터셋이다




## 학습 로그

가중이동평균 (WMA)를 구하는것에 상당히 애를 먹었다. 월별이나 일별 매출을 구한 뒤 단순 이동평균을 구하는 것은 쉬웠으나, 더 가까운 일자에 더 큰 가중치를 주는 가중이동평균은 sql만으로 하기에는 굉장히 번거로웠다. (매출 테이블을 원하는 기간만큼 추출하기 위한 셀프조인, 그 기간들에 가중치를 적용하기 위한 casewhen 등)
그래서 월별매출 테이블을 pandas와 연동 후 가중 이동평균을 구하는 방법을 찾아보았지만, 덜 복잡하긴 하지만 여전히 번거로웠다. (rolling을 통해 기간만큼 추출뒤 apply를 통해 weight 부여)

fanchart라고 하는 그래프를 만들기 위하여 쿼리를 작성하던 중, first_value() 라는 함수를 알게되었다. 그동안 그룹별로 처음 값을 구하기 위해 row_number을 파티션으로 나누고 1인값을 불러오는 식으로 작성을 해왔었는데, first_value에 기존 윈도우 함수를 작성하듯이 파티션별로 나누면 바로 구할 수 있다는 걸 알게되었다. 

보통 피벗테이블을 만들고자 한다면, 데이터를 불러온 뒤 파이썬의 판다스에서 group by나 pivot_table을 통하여 만들어 왔었는데, 문득 sql을 이용해서는 어떻게 만드는지 궁금해져서 알아보았다. group by를 한 뒤에 집계 함수를 사용할 때 case when을 사용하면 되는 것이였다. (여전히 파이썬이 더 편하다)

sql 학습을 마치고 공부할 예정인 추천 알고리즘의 일부분을 sql로 구현해 보았다. 어떤 특정한 상품을 주문했을 때 같이 주문된 상품들중 빈도수가 가장 많은 제품을 골라내는 쿼리와, 특정 상품을 주문했을 때 그 유저가 같이 주문했던 제품들중 빈도수가 가장 많은 제품을 골라내는 쿼리를 작성하였다. 

RFM 분석에 대하여 학습을 하였다. RFM 분석은 고객이 얼마나 최근에 (Recency), 얼마나 자주 (Frequency), 얼마나 많은 금액 (Monetary)를 사용했는지에 따라 고객을 분류하는 기법으로, 이전에 오퍼레이션스 매니지먼트라는 경영학과 과목을 수강했을 때 배운적이 있던 내용이였다. 이 과목에서 배웠던 내용으로는 기업에서 신규 소비자를 끌어들이는데 필요한 비용은 기존 고객들을 유지하는 비용보다 매우 많은 비용이 소모되므로 기존 고객들을 유지시킬 수 있는 방법을 모색하는 것이 중요하다는 것을 배웠었는데, 여기서 RFM분석을 통하여 이전에는 자주 높은 금액으로 주문한 고객들을 찾아내어 혜택같은 것을 줌으로써 다시 이 기업의 소비자로 돌아오게 한다던지 하는 방법을 사용할 수 있다.

가장 최근의 주문으로부터 며칠이 지났는지 구한 뒤에, 이 날짜들을 5분위로 나누어 frequency 등급을 측정해주었다. 하지만 최근에만 주문을 덜 했을 뿐이지, 평소에 주문을 매우 빈번하게 한 경우도 있을 것이라고 생각하여 그 이전에 다음 주문들까지의 평균 기간들을 구한 뒤 역시 분위로 나누었다. 
어차피 머신러닝이나 딥러닝에서 이런 데이터들을 사용할 때에는 모델에서 학습 과정중에 알아서 등급으로 분류하거나 특징들을 추출할테니 의미는 없을 수도 있지만, 그 이전에 직관성을 높이기 위해 시각화를 하기 위한 준비 과정이라 생각하고 코드를 작성해 보았다. 처음에는 ntile을 이용하여 간단하게 등급을 분류하고자 하였으나, 바로 문제가 발생하였다.
Frequency Rank를 부여하는 과정에서 현재 사용하고 있는 데이터의 기간이 2달 정도로 짧기 때문에 (용량 관계상),  대부분의 고객들의 주문 횟수는 1번이나 2번에서 그쳤고 그에 따라 ntile을 쓰면 ntile은 같은 크기의 순위 그룹으로 분류하는 것이기 때문에 매우 극단적으로 분류가 될 뿐더러, 같은 주문횟수인데도 다른 등급으로 분류되는 불상사도 생기게 된다. 
따라서 ntile을 이용하는 방법은 배제하고, 서브쿼리를 통하여 recency, frequency, monetary 각각에 대한 등급 기준 표를 만들고 이에 따라 분류하기로 하였다.
그 뒤 전체 등급을 배정한 뒤, 트리맵을 통하여 분포를 파악하기 쉽도록 하였다.

주문 데이터에 이어서 로그 데이터를 분석해 보기로 하였다. 이전에 참여했던 대회에서도 로그 데이터를 만져볼 기회는 있었지만, 그 때 당시 로그 데이터는 모든 데이터를 제공 받은게 아니라 일정 분량의 데이터만 제공 받았었기에 분석 도중 머신러닝 모델에 활용하기에는 적합하지 않다고 판단하여 참고용으로만 사용한 기억이 있다.
웹 프로그래밍을 해보았거나 학습을 한 경험이 없기에 가장 기초적인 내용을 먼저 학습을 하였다. 유저가 웹페이지에 접속을 할 때 세션이 생성이 된 후 일정 시간이 지나면 세션이 끝나게 되고, 다음에 접속을 할 때 또 세션이 생성되는 방식인 것 같았다. 즉, 유저별로 세션은 여러개 있을 수 있고, 각 세션별로 한 행동들이 sess_hits라는 테이블에 따로 저장이 되어있었다.

dau, wau, mau를 구하되 사용자가 지정한 날짜를 기준으로 구하는 방법에 대하여 학습을 하였다. 이전에 하던데로 date_trunc를 이용하여 쉽게 구할 수 있지만, 이건 데이터가 기본적으로 쌓여있는 곳에서 정해진 기간 틀에서 구하는 방식이라 문제가 있다. (wau를 구할때의 예시로 들어보면, 일요일부터 다음주 일요일 전까지의 유저 수를 구하거나, 혹은 어떠한 날짜가 주어졌을 때 그 날짜까지 일주일동안의 유저 수를 구하는 것을 date_trunc로는 불가능하다.) 따라서, 바인드 변수 (:column)을 이용하여 원하는 날짜를 입력받고 interval을 통하여 wau나 mau를 구하는 방식을 사용하였다.
